<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>论文笔记||Attention Is All You Need &mdash; Bird Nest</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/collection.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/common.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/css/posts/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/mzlogin/rouge-themes@master/dist/github.css"><link rel="canonical" href="https://uangjw.github.io/2022/02/07/paper-transformer/"><link rel="alternate" type="application/atom+xml" title="Bird Nest" href="https://uangjw.github.io"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/favicon.ico"><meta property="og:title" content="论文笔记||Attention Is All You Need"><meta name="keywords" content="Transformer"><meta name="og:keywords" content="Transformer"><meta name="description" content="Transformer"><meta name="og:description" content="Transformer"><meta property="og:url" content="https://uangjw.github.io/2022/02/07/paper-transformer/"><meta property="og:site_name" content="Bird Nest"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2022-02-07"> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery-ui.js"></script> <script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://uangjw.github.io/" title="Bird Nest"><span class="octicon octicon-mark-github"></span> Bird Nest</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://uangjw.github.io/" class=" site-header-nav-item" target="" title="Home">Home</a> <a href="https://uangjw.github.io/categories/" class=" site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://uangjw.github.io/about/" class=" site-header-nav-item" target="" title="About">About</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="论文笔记||Attention"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">论文笔记||Attention Is All You Need</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2022/02/07 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#PaperNotes" title="PaperNotes">PaperNotes</a> </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://uangjw.github.io/categories/#Deep Learning" title="Deep Learning">Deep Learning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 1446 字，约 5 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h3 id="transformer">Transformer</h3><p>Music Transformer的模型建立在Transformer模型的基础之上。</p><p>论文：Attention Is All You Need</p><p>参考博客：https://blog.csdn.net/longxinchen_ml/article/details/86533005</p><p>Transformer模型首次于文章Attention Is All You Need提出，如题目所述，模型仅仅依赖注意力机制来表达输入输出之间的全局依赖，完全摒弃了循环与卷积。相对于具有相同性能的其他模型，Transformer更可并行化，且需求的训练时间更短。</p><h4 id="模型架构">模型架构</h4><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/transformer1.png" style="zoom: 80%;" /></p><p>编码器：6个相同的网络层组成，其中每层都有两个子网络层（或者理解成6个编码器级联）。子层中按传输顺序第一个是一个Multi-Head self-attention机制，即一个自注意力层。第二个子层是一个简单的全连接前馈网络层。</p><p>解码器：同样由6个相同的网络层组成，除了与编码器相同的两个子层，解码器中还有一个对编码器的输出执行multi-head attention的子层。</p><p>编码器与解码器的层与层之间都采用残差连接，并对每层输出进行归一化（这两步即图中Add&amp;Norm，将层输出与输入相加并归一化，目的是应对梯度消失以及防止过拟合）</p><h4 id="自注意力机制">自注意力机制</h4><p>注意力函数（attention function）可以看作是一个从一个查询与一组键值对到一个输出的映射，其中查询、键、值以及输出都是向量，它们的创建都是通过一个权值矩阵与待处理的向量相乘实现的。在NLP情形下，可以认为自注意力机制就是在模型处理输入序列的每个单词时能够关注到整个序列中的所有单词，从而更好地编/解码；具体来说，当我们希望查询某单词在句子的其他位置上的“注意力”，我们就将其他位置上的单词拿来对该单词做“注意力计算（打分）”，计算结果决定了编码该单词的过程中有多重视句子的其它部分。</p><p>作者使用的Scaled Dot-Product Attention机制的结构如下图。计算相应的attention值（输出）的流程可以表达为如下公式： \(Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\) 其中$Q$即请求向量，$K$即所有键值组成的向量，$V$即所有键对应值的向量，$d_k$即键值对的维数。这种计算attention的方式是在一般的dot-product attention的基础上多除了一个维数的平方根，这一步可以让梯度更稳定。将注意力分数计算完毕后通过softmax函数来传递结果，这一步使得所有单词的分数都归一化，得到的分数都是正值且和为1。</p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/transformer3.png" style="zoom: 80%;" /></p><p>前面描述模型结构的时候提到实际上用到的是Multi-Head Attention机制（多头注意力机制）。多头注意力机制扩展了模型专注于输入序列不同位置的能力，给出了注意力层的多个“表示子空间”（论文的表述：“能够联合地从不同的表示子空间中关注不同位置的信息”）。结合下图理解多头注意力机制，向量VKQ并行地进行h次线性投影（linear projection，通过矩阵乘法进行），投影结果将被并行地进行scaled dot-product注意力分数计算，然后拼接到一起再被线性投影，得到一个矩阵。公式表达为： \(MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\ where \ head_i=Attention(QW_i^Q,KW_I^K,VW_i^V)\) <br /></p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/transformer2.png" style="zoom: 80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/images/transformer4.png" div="" align="center" /></p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://uangjw.github.io" target="_blank">Jingwen Huang</a></li><li>本文链接：<a href="https://uangjw.github.io/2022/02/07/paper-transformer/" target="_blank">https://uangjw.github.io/2022/02/07/paper-transformer/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://uangjw.github.io/assets/search_data.json', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2021 <span title="Jingwen Huang">Jingwen Huang</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/uangjw/uangjw.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://uangjw.github.io/" title="Home" target="">Home</a></li><li> <a href="https://uangjw.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://uangjw.github.io/about/" title="About" target="">About</a></li><li><a href="https://uangjw.github.io"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul><script async src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/vendor/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="mobile-hidden" style="margin-top:8px"> <span id="busuanzi_container_site_pv" style="display:none"> 本站访问量<span id="busuanzi_value_site_pv"></span>次 </span> <span id="busuanzi_container_site_uv" style="display:none"> / 本站访客数<span id="busuanzi_value_site_uv"></span>人 </span> <span id="busuanzi_container_page_pv" style="display:none"> / 本页访问量<span id="busuanzi_value_page_pv"></span>次 / 统计始于2021-03-10 </span></div></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://cdn.jsdelivr.net/gh/uangjw/uangjw.github.io@master/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script><div style="display:none"> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-80669434-1', 'auto'); ga('send', 'pageview'); </script></div></body></html>
